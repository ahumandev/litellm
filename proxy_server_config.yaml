model_list:
  - model_name: github_copilot/claude-haiku-4.5
    litellm_params:
      model: github_copilot/claude-haiku-4.5
      custom_llm_provider: github_copilot
      supports_response_schema: false
    model_info:
      supports_responses_api: false
  - model_name: github_copilot/claude-sonnet-4.5
    litellm_params:
      model: github_copilot/claude-sonnet-4.5
      custom_llm_provider: github_copilot
      supports_response_schema: false
    model_info:
      supports_responses_api: false
  - model_name: github_copilot/claude-opus-4.5
    litellm_params:
      model: github_copilot/claude-opus-4.5
      custom_llm_provider: github_copilot
      supports_response_schema: false
    model_info:
      supports_responses_api: false
  - model_name: github_copilot/gemini-2.5-pro
    litellm_params:
      model: github_copilot/gemini-2.5-pro
      custom_llm_provider: github_copilot
      supports_response_schema: false
    model_info:
      supports_responses_api: false
  - model_name: github_copilot/gpt-5-mini
    litellm_params:
      model: github_copilot/gpt-5-mini
      custom_llm_provider: github_copilot
      supports_response_schema: false
    model_info:
      supports_responses_api: false
  - model_name: github_copilot/gpt-5.2
    litellm_params:
      model: github_copilot/gpt-5.2
      custom_llm_provider: github_copilot
      supports_response_schema: false
    model_info:
      supports_responses_api: false

  # Allow Claude Code to use LiteLLM
  - model_name: claude-sonnet-4-5-20250929
    litellm_params:
      model: github_copilot/claude-sonnet-4.5
      api_key: "os.environ/GITHUB_TOKEN" # or however you've configured it
  - model_name: claude-haiku-4-5-20251001
    litellm_params:
      model: github_copilot/claude-haiku-4.5
      api_key: "os.environ/GITHUB_TOKEN"
  - model_name: claude-opus-4-5-20251101
    litellm_params:
      model: github_copilot/claude-opus-4.5
      api_key: "os.environ/GITHUB_TOKEN"

litellm_settings:
  # set_verbose: True # Uncomment this if you want to see verbose logs; not recommended in production
  drop_params: True
  success_callback: []
  # max_budget: 100
  # budget_duration: 30d
  num_retries: 0
  request_timeout: 600
  telemetry: False
  # context_window_fallbacks: [{ "gpt-3.5-turbo": ["gpt-3.5-turbo-large"] }]
  # default_team_settings:
  #   - team_id: team-1
  #     success_callback: ["langfuse"]
  #     failure_callback: ["langfuse"]
  #     langfuse_public_key: os.environ/LANGFUSE_PROJECT1_PUBLIC # Project 1
  #     langfuse_secret: os.environ/LANGFUSE_PROJECT1_SECRET # Project 1
  #   - team_id: team-2
  #     success_callback: ["langfuse"]
  #     failure_callback: ["langfuse"]
  #     langfuse_public_key: os.environ/LANGFUSE_PROJECT2_PUBLIC # Project 2
  #     langfuse_secret: os.environ/LANGFUSE_PROJECT2_SECRET # Project 2
  #     langfuse_host: https://us.cloud.langfuse.com
  # cache: true   # [OPTIONAL] use for caching responses
  # enable_caching_on_provider_specific_optional_params: True  # Include provider-specific params in cache keys
  # cache_params:  # And for shared health check
  #   type: redis
  #   host: localhost
  #   port: 6379

# For /fine_tuning/jobs endpoints
# finetune_settings:
#   - custom_llm_provider: azure
#     api_base: os.environ/AZURE_API_BASE
#     api_key: os.environ/AZURE_API_KEY
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

# # for /files endpoints
# files_settings:
#   - custom_llm_provider: azure
#     api_base: os.environ/AZURE_API_BASE
#     api_key: os.environ/AZURE_API_KEY
#     api_version: "2023-03-15-preview"
#   - custom_llm_provider: openai
#     api_key: os.environ/OPENAI_API_KEY

# router_settings:
#   routing_strategy: usage-based-routing-v2
#   redis_host: os.environ/REDIS_HOST
#   redis_password: os.environ/REDIS_PASSWORD
#   redis_port: os.environ/REDIS_PORT
#   enable_pre_call_checks: true
#   model_group_alias:
#     { "my-special-fake-model-alias-name": "fake-openai-endpoint-3" }

general_settings:
  master_key: sk-1234 # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  database_connection_pool_limit: 10
  # background_health_checks: true
  # use_shared_health_check: true
  # health_check_interval: 30
  # database_url: "postgresql://<user>:<password>@<host>:<port>/<dbname>" # [OPTIONAL] use for token-based auth to proxy

  # pass_through_endpoints:
  #   - path: "/v1/rerank" # route you want to add to LiteLLM Proxy Server
  #     target: "https://api.cohere.com/v1/rerank" # URL this route should forward requests to
  #     headers: # headers to forward to this URL
  #       content-type: application/json # (Optional) Extra Headers to pass to this endpoint
  #       accept: application/json
  #     forward_headers: True

# environment_variables:
# settings for using redis caching
# REDIS_HOST: redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com
# REDIS_PORT: "16337"
# REDIS_PASSWORD:
